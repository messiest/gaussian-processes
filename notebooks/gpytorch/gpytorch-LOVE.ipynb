{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Predictive Distributions (LOVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will provide an overview of fast computation of variances and sampling using the LOVE algorithm, introduced [here](https://arxiv.org/abs/1803.06058).\n",
    "Using LOVE can significantly reduce the computational costs of computing predictive distributions.\n",
    "This can be especially useful in settings like small-cale Bayesian optimization, where predictions need to be made at a large number of candidate points, but there aren't enough training examples to warrant the use of sparse GP methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tnrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will train an exact GP on the `skillcraft` UCI dataset and compare the time required to make predictions with each model.\n",
    "The code below will download a copy of the dataset that has been preprocessed (scaled and normalized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'skillcraft' UCI dataset...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "DATA = os.path.join('data/skillcraft.mat')\n",
    "\n",
    "if not os.path.isfile(DATA):  # .mat is a MatLab file\n",
    "    print(\"Downloading \\'skillcraft\\' UCI dataset...\")\n",
    "    url = 'https://drive.google.com/uc?export=download&id=1xQ1vgx_bOsLDQ3RLbJwPxMyJHW7U9Eqd'\n",
    "    urllib.request.urlretrieve(url, DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we'll simply split the dataset using the first 40% for training and the remaining 60% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat  # MatLab file loader\n",
    "\n",
    "\n",
    "data = torch.Tensor(loadmat(DATA)['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n = int(math.floor(0.4*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous()  # .cuda()  # not using cuda here...\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the GP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that we'll use here is essentially the same as in `gpytorch-regression.ipynb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()  # once again, no .cuda()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params': model.parameters()}],  # includes likelihood parameters\n",
    "    lr=0.1,\n",
    ")\n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)  # loss\n",
    "\n",
    "\n",
    "def train(n):  # convenient for time tests\n",
    "    pbar = tnrange(n)\n",
    "    for i in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -1 * mll(output, train_y)\n",
    "        loss.backward()\n",
    "        \n",
    "        print(f\"{i+1:3d}/{n:3d} | Loss: {loss.item(): .3f}\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ebc824db404b9c9a9b548179d29b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/100 | Loss:  1.060\n",
      "  2/100 | Loss:  0.995\n",
      "  3/100 | Loss:  0.936\n",
      "  4/100 | Loss:  0.878\n",
      "  5/100 | Loss:  0.821\n",
      "  6/100 | Loss:  0.773\n",
      "  7/100 | Loss:  0.719\n",
      "  8/100 | Loss:  0.672\n",
      "  9/100 | Loss:  0.623\n",
      " 10/100 | Loss:  0.579\n",
      " 11/100 | Loss:  0.539\n",
      " 12/100 | Loss:  0.493\n",
      " 13/100 | Loss:  0.455\n",
      " 14/100 | Loss:  0.413\n",
      " 15/100 | Loss:  0.379\n",
      " 16/100 | Loss:  0.344\n",
      " 17/100 | Loss:  0.311\n",
      " 18/100 | Loss:  0.278\n",
      " 19/100 | Loss:  0.248\n",
      " 20/100 | Loss:  0.221\n",
      " 21/100 | Loss:  0.199\n",
      " 22/100 | Loss:  0.174\n",
      " 23/100 | Loss:  0.152\n",
      " 24/100 | Loss:  0.137\n",
      " 25/100 | Loss:  0.120\n",
      " 26/100 | Loss:  0.105\n",
      " 27/100 | Loss:  0.096\n",
      " 28/100 | Loss:  0.087\n",
      " 29/100 | Loss:  0.079\n",
      " 30/100 | Loss:  0.078\n",
      " 31/100 | Loss:  0.075\n",
      " 32/100 | Loss:  0.076\n",
      " 33/100 | Loss:  0.078\n",
      " 34/100 | Loss:  0.079\n",
      " 35/100 | Loss:  0.086\n",
      " 36/100 | Loss:  0.081\n",
      " 37/100 | Loss:  0.088\n",
      " 38/100 | Loss:  0.085\n",
      " 39/100 | Loss:  0.088\n",
      " 40/100 | Loss:  0.088\n",
      " 41/100 | Loss:  0.089\n",
      " 42/100 | Loss:  0.089\n",
      " 43/100 | Loss:  0.090\n",
      " 44/100 | Loss:  0.083\n",
      " 45/100 | Loss:  0.086\n",
      " 46/100 | Loss:  0.079\n",
      " 47/100 | Loss:  0.085\n",
      " 48/100 | Loss:  0.082\n",
      " 49/100 | Loss:  0.078\n",
      " 50/100 | Loss:  0.082\n",
      " 51/100 | Loss:  0.074\n",
      " 52/100 | Loss:  0.067\n",
      " 53/100 | Loss:  0.079\n",
      " 54/100 | Loss:  0.077\n",
      " 55/100 | Loss:  0.073\n",
      " 56/100 | Loss:  0.074\n",
      " 57/100 | Loss:  0.073\n",
      " 58/100 | Loss:  0.072\n",
      " 59/100 | Loss:  0.071\n",
      " 60/100 | Loss:  0.074\n",
      " 61/100 | Loss:  0.071\n",
      " 62/100 | Loss:  0.072\n",
      " 63/100 | Loss:  0.071\n",
      " 64/100 | Loss:  0.073\n",
      " 65/100 | Loss:  0.075\n",
      " 66/100 | Loss:  0.077\n",
      " 67/100 | Loss:  0.072\n",
      " 68/100 | Loss:  0.069\n",
      " 69/100 | Loss:  0.074\n",
      " 70/100 | Loss:  0.069\n",
      " 71/100 | Loss:  0.070\n",
      " 72/100 | Loss:  0.077\n",
      " 73/100 | Loss:  0.071\n",
      " 74/100 | Loss:  0.070\n",
      " 75/100 | Loss:  0.070\n",
      " 76/100 | Loss:  0.071\n",
      " 77/100 | Loss:  0.071\n",
      " 78/100 | Loss:  0.071\n",
      " 79/100 | Loss:  0.070\n",
      " 80/100 | Loss:  0.072\n",
      " 81/100 | Loss:  0.070\n",
      " 82/100 | Loss:  0.071\n",
      " 83/100 | Loss:  0.073\n",
      " 84/100 | Loss:  0.073\n",
      " 85/100 | Loss:  0.073\n",
      " 86/100 | Loss:  0.074\n",
      " 87/100 | Loss:  0.070\n",
      " 88/100 | Loss:  0.071\n",
      " 89/100 | Loss:  0.072\n",
      " 90/100 | Loss:  0.067\n",
      " 91/100 | Loss:  0.070\n",
      " 92/100 | Loss:  0.070\n",
      " 93/100 | Loss:  0.069\n",
      " 94/100 | Loss:  0.068\n",
      " 95/100 | Loss:  0.070\n",
      " 96/100 | Loss:  0.073\n",
      " 97/100 | Loss:  0.076\n",
      " 98/100 | Loss:  0.074\n",
      " 99/100 | Loss:  0.072\n",
      "100/100 | Loss:  0.070\n",
      "\n",
      "CPU times: user 54.8 s, sys: 14 s, total: 1min 8s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%time train(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we will examine how long it takes to generate predictions using the standard SKI testing code, without any form of acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Exact Predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model.eval()  # evaluation mode for inference\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():  # standard SKI testing code\n",
    "    start = time.time()\n",
    "    preds = likelihood(model(test_x))\n",
    "    exact_covar = preds.covariance_matrix\n",
    "    exact_covar_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute exact mean and covariances: 3.36secs\n"
     ]
    }
   ],
   "source": [
    "print(f'Time to compute exact mean and covariances: {exact_covar_time:.2f}secs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get comparable timing results, we're going to perform some garbage cleanup with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()  # probably not needed since we're not using CUDA\n",
    "    model.train()\n",
    "    likelihood.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Predictions with LOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute predictions with LOVE, utilize the context manager `gpytorch.fast_pred_var()`.\n",
    "You can also change the settings for LOVE with additional context managers.\n",
    "For example, using `gpytorch.settings.max_root_decomposition_size(35)` affects the accuracy of the LOVE solver (passing larger values increases accuracy, but slows computation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.fast_pred_var(), gpytorch.settings.max_root_decomposition_size(25):\n",
    "    start = time.time()\n",
    "    preds_love = likelihood(model(test_x))\n",
    "    fast_covar = preds_love.covariance_matrix\n",
    "    fast_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute exact mean and covariances: 3.36secs\n",
      "Time to compute mean and covariances with LOVE: 1.71secs\n"
     ]
    }
   ],
   "source": [
    "print(f'Time to compute exact mean and covariances: {exact_covar_time:.2f}secs')\n",
    "print(f'Time to compute mean and covariances with LOVE: {fast_time:.2f}secs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Predictions with LOVE using cached results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell computed the caches required to get fast predicitons.\n",
    "Once this is done we are able to use these cached results to make predictions _extremely_ fast (so long as we don't put our model back in training mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(), gpytorch.fast_pred_var(), gpytorch.settings.max_root_decomposition_size(25):\n",
    "    start = time.time()\n",
    "    preds_love_cache = likelihood(model(test_x))\n",
    "    fast_covar_cache = preds_love_cache.covariance_matrix\n",
    "    fast_time_cache = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute exact mean and covariances: 3.36secs\n",
      "Time to compute mean and covariances with LOVE: 1.71secs\n",
      "Time to compute mean and covariances with LOVE using cache: 1.21secs\n"
     ]
    }
   ],
   "source": [
    "print(f'Time to compute exact mean and covariances: {exact_covar_time:.2f}secs')\n",
    "print(f'Time to compute mean and covariances with LOVE: {fast_time:.2f}secs')\n",
    "print(f'Time to compute mean and covariances with LOVE using cache: {fast_time_cache:.2f}secs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Error Between Exact and Fast Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error between exact covar matrix and fast covar matrix: 0.0005233271\n"
     ]
    }
   ],
   "source": [
    "mae = (exact_covar - fast_covar).abs().mean()\n",
    "print(f'Mean Absolute Error between exact covar matrix and fast covar matrix: {mae:.10f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
