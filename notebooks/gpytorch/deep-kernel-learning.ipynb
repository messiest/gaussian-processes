{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Kernel Learning on CIFAR10/100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we'll demonstrate how to train a medium sized DenseNet on either of the CIFAR datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import gpytorch\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "crop = transforms.RandomCrop(32, padding=4)\n",
    "flip = transforms.RandomHorizontalFlip()\n",
    "common_trans = [transforms.ToTensor(), normalize]\n",
    "\n",
    "train_compose = transforms.Compose([crop, flip] + common_trans)\n",
    "test_compose = transforms.Compose(common_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cifar10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'cifar10':\n",
    "    d_func = datasets.CIFAR10\n",
    "    train_set = datasets.CIFAR10('data', train=True, transform=train_compose, download=True)\n",
    "    test_set = datasets.CIFAR10('data', train=False, transform=test_compose)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=3, pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True, num_workers=3, pin_memory=True)\n",
    "    num_classes = 10\n",
    "elif dataset == 'cifar100':\n",
    "    d_func = datasets.CIFAR100\n",
    "    train_set = datasets.CIFAR100('data', train=True, transform=train_compose, download=True)\n",
    "    test_set = datasets.CIFAR100('data', train=False, transform=test_compose)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=3, pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True, num_workers=3, pin_memory=True)\n",
    "    num_classes = 100\n",
    "else:\n",
    "    raise RuntimeError('dataset must be either \"cifar10\" or \"cifar100\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Kernel Learning (DKL) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded, we begin defining the DKL model.\n",
    "A DKL model consists of three parts:\n",
    "1. The Neural Network\n",
    "2. The Gaussian Process Layer\n",
    "3. Softmax Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from densenet import DenseNet\n",
    "\n",
    "\n",
    "class DenseNetFeatureExtractor(DenseNet):\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.avg_pool2d(out, kernel_size=self.avgpool_size).view(features.size(0), -1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = DenseNetFeatureExtractor(block_config=(6, 6, 6), num_classes=num_classes)\n",
    "num_features = feature_extractor.classifier.in_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the GP Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcessLayer(gpytorch.models.AdditiveGridInducingVariationalGP):\n",
    "    def __init__(self, num_dim, grid_bounds=(-10., 10.), grid_size=128):\n",
    "        super(GaussianProcessLayer, self).__init__(\n",
    "                grid_size=grid_size,\n",
    "                grid_bounds=[grid_bounds],\n",
    "                num_dim=num_dim,\n",
    "                mixing_params=False,\n",
    "                sum_output=False,\n",
    "            )\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.cov_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                log_lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(\n",
    "                    math.exp(-1),\n",
    "                    math.exp(1),\n",
    "                    sigma=0.1,\n",
    "                    log_transform=True,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.grid_bounds = grid_bounds\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        cov = self.cov_module(x)\n",
    "        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, feature_extractor, num_dim, grid_bounds=(-10., 10.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.gp_layer = GaussianProcessLayer(num_dim=num_dim, grid_bounds=grid_bounds)\n",
    "        self.grid_bounds = grid_bounds\n",
    "        self.num_dim = num_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = gpytorch.utils.grid.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        res = self.gp_layer(features)\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DKLModel(feature_extractor, num_dim=num_features)\n",
    "likelihood = gpytorch.likelihoods.SoftmaxLikelihood(num_features=model.num_dim, n_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "lr = 0.1\n",
    "optimizer = SGD(\n",
    "    [{'params': model.feature_extractor.parameters(), 'lr': lr},\n",
    "     {'params': model.gp_layer.parameters(), 'lr': 0.1 * lr},\n",
    "     {'params': likelihood.parameters(), 'lr': lr}],\n",
    "    lr=lr,\n",
    "    momentum=0.9,\n",
    "    nesterov=True,\n",
    "    weight_decay=0,\n",
    ")\n",
    "scheduler = MultiStepLR(optimizer, milestones=[0.5 * n_epochs, 0.75 * n_epochs], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, lr=0.1):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    mll = gpytorch.mlls.VariationalMarginalLogLikelihood(likelihood, model, num_data=len(train_loader.dataset))\n",
    "    train_loss = 0.\n",
    "    pbar = tqdm(train_loader)\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "#         data, target = data.cuda(), target.cuda()  # no CUDA!\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = -1 * mll(output, target)\n",
    "        loss.backward()\n",
    "        if (batch_idx+1) % 50 == 0:\n",
    "            print(f'Epoch: {epoch} | {batch_idx+1:3d}/{len(train_loader):3d}, Loss: {loss.item(): .6f}')\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    pbar = tqdm(test_loader)\n",
    "    for data, target in pbar:\n",
    "#         data, target = data.cuda(), target.cuda()  # no CUDA!\n",
    "        with torch.no_grad():\n",
    "            output = likelihood(model(data))\n",
    "            pred = output.argmax()\n",
    "            correct += pred.eq(target.view_as(pred)).cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)   \n",
    "    print(f'Test Set | Average Loss: {test_loss:.4f}, Accuracy: {100. * correct / len(tes_loader.dataset):.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    scheduler.step()\n",
    "    \n",
    "    with gpytorch.settings.use_toeplitz(False), gpytorch.settings.max_preconditioner_size(0):\n",
    "        train(epoch)\n",
    "        test()\n",
    "        \n",
    "    stat_dict = model.state_dict()\n",
    "    likelihood_state_dict = likelihood.state_dict()\n",
    "    \n",
    "    torch.save(\n",
    "        {\n",
    "            'model': state_dict,\n",
    "            'likelihood': likelihood_state_dict\n",
    "        },\n",
    "        'dkl_cifar_checkpoint.dat',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
